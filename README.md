# GPT-2-From-Scratch
An implementation of the complete GPT-2 (124M) model architecture and pre-training loop from scratch using PyTorch.
