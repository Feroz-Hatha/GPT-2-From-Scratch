# GPT-2 Architecture + Pre-Training Using PyTorch
This repository contains a complete PyTorch implementation of the 124M GPT-2 architecture, including tokenization, embeddings, masked multi-head attention, transformer blocks (layer norm, GELU, residuals), and it's full pretraining loop with cross-entropy loss and perplexity evaluation.
